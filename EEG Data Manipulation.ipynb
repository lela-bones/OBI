{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Data Manipulation\n",
    "### By: Lela Bones and Adam Jump\n",
    "The purpose of this journal is to view our dataset of EEG Data and manipulate the data into the format that we want.\n",
    "\n",
    "In regards to our research, we want to be able to make software that uses a close-looped system to allow a user to control a robotic arm with their mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#best file= mac_dude_BlinkTest_1.json\n",
    "#second best = a88ac021f60d23d32aec7764199fcfcf64c3784548eedc852c90f6a4baa24f48.json\n",
    "filename = open('data/mac_dude_BlinkTest_1.json', 'r')\n",
    "\n",
    "#loads json file\n",
    "data = json.load(filename)\n",
    "#stores only the eeg data\n",
    "eeg = np.array(data['patterns'])\n",
    "\n",
    "#maps the node number to the data\n",
    "inputs = map(lambda p: p['input'], eeg[2:])\n",
    "#puts data into a list\n",
    "inputs = list(inputs)\n",
    "#puts data into an array\n",
    "inputs = np.array(inputs)\n",
    "#maps the node number to the data\n",
    "outputs = map(lambda p: p['output'], eeg[2:])\n",
    "#puts data into a list\n",
    "outputs = list(outputs)\n",
    "#puts data into an array\n",
    "outputs = np.array(outputs)\n",
    "#turns into dataframe\n",
    "data = np.append(inputs, outputs, 1)\n",
    "df = pd.DataFrame(data=data)\n",
    "torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6162.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "data.shape[0]/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Filtering the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of EEG Data Filtering\n",
    "According to a few sources, we believe that a FFT based filter would be the best. There are a couple of ways to do this, Finite Impulse Response (FIR) is one way. Using the SyPy library we decided from this [readthedocs](https://scipy-cookbook.readthedocs.io/items/ApplyFIRFilter.html) that signal.fftconvolve would be the fastest/most stable method of performing an FIR on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#Initialized the class for the LSTM\n",
    "# nn.Module is a built in base class for all nueral network models\n",
    "class Sequence(nn.Module):\n",
    "    #Initializes the LSTM\n",
    "    def __init__(self, hidden_size):\n",
    "        #We Intialize with super so we have access to the nn.Module\n",
    "        super(Sequence, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Initializes an LSTMCell(input_size, hidden_size)\n",
    "        self.lstm1 = nn.LSTMCell(1, self.hidden_size) #We are training the first space \n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_size, self.hidden_size) #We are training the first half\n",
    "        #Applies a linear transformation to the data of y=x(A^T)+b\n",
    "        self.linear = nn.Linear(self.hidden_size, 1) #We are tranforming lstm2 into a linear wave\n",
    "    \n",
    "    #This is the feed-forward function where we default our prediction to 0\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = [] #We initialize the array for our output\n",
    "        #Initializing the hidden state(batch, hidden_size) for each element in the batch\n",
    "        #It is defaulted to 0 because it wasn't provided \n",
    "        h_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.double)\n",
    "        #Initializing the cell state(batch, hidden_size) for each element in the batch\n",
    "        #It is also defaulted to 0 if not provided\n",
    "        c_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.double)\n",
    "        #Storing the next hidden state (batch, hidden_size) for each element in the batch\n",
    "        h_t2 = torch.zeros(input.size(0), self.hidden_size, dtype=torch.double)\n",
    "        ##Storing the next cell state (batch, hidden_size) for each element in the batch\n",
    "        c_t2 = torch.zeros(input.size(0), self.hidden_size, dtype=torch.double)\n",
    "    \n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "                    h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "                    h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "                    output = self.linear(h_t2)\n",
    "                    outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f901aab53fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set random seed to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load data and make training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'traindata.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# set random seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# load data and make training set\n",
    "data = torch.load('traindata.pt')\n",
    "input = torch.from_numpy(data[6162:, :7])\n",
    "target = torch.from_numpy(data[6162:, 8])\n",
    "test_input = torch.from_numpy(data[:6162, :7])\n",
    "test_target = torch.from_numpy(data[:6162, 8])\n",
    "# build the model\n",
    "seq = Sequence(50)\n",
    "seq.double()\n",
    "criterion = nn.MSELoss()\n",
    "# use LBFGS as optimizer since we can load the whole data to train\n",
    "optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "#begin to train\n",
    "for i in range(100):\n",
    "    print('STEP: ', i)\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = seq.forward(input)\n",
    "        loss = criterion(out, target)\n",
    "        print('loss:', loss.item())\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    optimizer.step(closure)\n",
    "    # begin to predict, no need to track gradient here\n",
    "    with torch.no_grad():\n",
    "        future = 1000\n",
    "        pred = seq(test_input, future=future)\n",
    "        loss = criterion(pred[:, :-future], test_target)\n",
    "        print('test loss:', loss.item())\n",
    "        y = pred.detach().numpy()\n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.savefig('predict%d.pdf'%i)\n",
    "    plt.close()\n",
    "#     checkpoint = {'n_hidden': seq.n_hidden,\n",
    "#               'n_layers': net.n_layers,\n",
    "#               'state_dict': net.state_dict(),\n",
    "#               'tokens': net.chars}\n",
    "#     with open('rnn.net', 'wb') as f:\n",
    "#         torch.save(,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Epoch [1/2], Step [100/600], Loss: 0.5941\n",
      "Epoch [1/2], Step [200/600], Loss: 0.4275\n",
      "Epoch [1/2], Step [300/600], Loss: 0.2043\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1226\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0515\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1214\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0225\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0461\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0118\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0416\n",
      "Epoch [2/2], Step [500/600], Loss: 0.1680\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0830\n",
      "Test Accuracy of the model on the 10000 test images: 97.54 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#Initialized the class for the LSTM\n",
    "# nn.Module is a built in base class for all nueral network models\n",
    "class Sequence(nn.Module):\n",
    "    #Initializes the LSTM\n",
    "    def __init__(self):\n",
    "        #We Intialize with super so we have access to the nn.Module\n",
    "        super(Sequence, self).__init__()\n",
    "        #Initializes an LSTMCell(input_size, hidden_size)\n",
    "        self.lstm1 = nn.LSTMCell(1, 51) #We are training the first space \n",
    "        self.lstm2 = nn.LSTMCell(51, 51) #We are training the first half\n",
    "        #Applies a linear transformation to the data of y=x(A^T)+b\n",
    "        self.linear = nn.Linear(51, 1) #We are tranforming lstm2 into a linear wave\n",
    "    \n",
    "    #This is the feed-forward function where we default our prediction to 0\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = [] #We initialize the array for our output\n",
    "        #Initializing the hidden state(batch, hidden_size) for each element in the batch\n",
    "        #It is defaulted to 0 because it wasn't provided \n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        #Initializing the cell state(batch, hidden_size) for each element in the batch\n",
    "        #It is also defaulted to 0 if not provided\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        #Storing the next hidden state (batch, hidden_size) for each element in the batch\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        ##Storing the next cell state (batch, hidden_size) for each element in the batch\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "    \n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "                    h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "                    h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "                    output = self.linear(h_t2)\n",
    "                    outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # set random seed to 0\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    # load data and make training set\n",
    "    data = torch.load('traindata.pt')\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "    # build the model\n",
    "    seq = Sequence()\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    # use LBFGS as optimizer since we can load the whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "    #begin to train\n",
    "    for i in range(15):\n",
    "        print('STEP: ', i)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        # begin to predict, no need to track gradient here\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = seq(test_input, future=future)\n",
    "            loss = criterion(pred[:, :-future], test_target)\n",
    "            print('test loss:', loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.savefig('predict%d.pdf'%i)\n",
    "    plt.close()\n",
    "#     checkpoint = {'n_hidden': seq.n_hidden,\n",
    "#               'n_layers': net.n_layers,\n",
    "#               'state_dict': net.state_dict(),\n",
    "#               'tokens': net.chars}\n",
    "#     with open('rnn.net', 'wb') as f:\n",
    "#         torch.save(,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
